{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BERT for text classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These notes will show you how to use BERT for text-classification. Some highlights:\n",
    "* dataloaders in pytorch,\n",
    "* BERT from pytorch-transformers,\n",
    "* freezing layers,\n",
    "* learning rate schedulers, optimizers and gradient clipping,\n",
    "* mixed precision training,\n",
    "* logging your training.\n",
    "\n",
    "Resources:\n",
    "* [a good tutorial from C. McCormick](https://mccormickml.com/2019/07/22/BERT-fine-tuning/)\n",
    "* [the original BERT paper](https://arxiv.org/abs/1810.04805)\n",
    "* [the main library we use is pytorch-tranformers from huggingface](https://github.com/huggingface/pytorch-transformers)\n",
    "* ['Fine-tuning BERT for text classification' by Sun et. al](https://arxiv.org/abs/1905.05583)\n",
    "* [a post on compressing and speeding-up large models](https://blog.rasa.com/compressing-bert-for-faster-prediction-2/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pytorch_transformers import *\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pickle, os, shutil\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tqdm import tqdm, trange, tqdm_notebook\n",
    "import io\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "from apex import amp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various pretrained models are available in pytorch-transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#          Model          | Tokenizer          | Pretrained weights shortcut\n",
    "MODELS = [(BertModel,       BertTokenizer,      'bert-base-uncased'),\n",
    "          (OpenAIGPTModel,  OpenAIGPTTokenizer, 'openai-gpt'),\n",
    "          (GPT2Model,       GPT2Tokenizer,      'gpt2'),\n",
    "          (TransfoXLModel,  TransfoXLTokenizer, 'transfo-xl-wt103'),\n",
    "          (XLNetModel,      XLNetTokenizer,     'xlnet-base-cased'),\n",
    "          (XLMModel,        XLMTokenizer,       'xlm-mlm-enfr-1024'),\n",
    "          (RobertaModel,    RobertaTokenizer,   'roberta-base')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and even BERT has many variations, the first being the base model and the rest are equipped with different heads for different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "BERT_MODEL_CLASSES = [BertModel, BertForPreTraining, BertForMaskedLM, BertForNextSentencePrediction,\n",
    "                      BertForSequenceClassification, BertForMultipleChoice, BertForTokenClassification,\n",
    "                      BertForQuestionAnswering]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadModelTokenizer(num_labels = 2, language = 'english'):\n",
    "    model_class = BertForSequenceClassification\n",
    "    tokenizer_class = BertTokenizer\n",
    "    if language == 'english':\n",
    "        pretrained_weights = 'bert-base-uncased'\n",
    "    if language == 'german':\n",
    "        pretrained_weights = 'bert-base-german-cased'\n",
    "\n",
    "    tokenizer = tokenizer_class.from_pretrained(pretrained_weights, do_lower_case=False)\n",
    "    model = model_class.from_pretrained(pretrained_weights, num_labels)\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The pre-trained model you are loading is an uncased model but you have set `do_lower_case` to False. We are setting `do_lower_case=True` for you but you may want to check this behavior.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = loadModelTokenizer(4, 'english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pass model to the GPU already:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting model parameters and accessing individual layers is not too bad using the following functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109483778"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model, trainable_only = True):\n",
    "    if trainable_only:\n",
    "        return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_layers(model, verbose=False):\n",
    "    print('Number of layers: {}'.format(len(list(model.children()))))\n",
    "    if verbose:\n",
    "        for c in model.children():\n",
    "            print(str(c)[:50]+'...')\n",
    "\n",
    "def get_child(model, *arg):\n",
    "    res = model\n",
    "    for i in arg:\n",
    "        res = list(res.children())[i]\n",
    "    return res\n",
    "\n",
    "def freeze_model(model):\n",
    "    for param in model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "def unfreeze_model(model):\n",
    "    for param in model.parameters():\n",
    "            param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 3\n",
      "BertModel(\n",
      "  (embeddings): BertEmbeddings(\n",
      "    (wo...\n",
      "Dropout(p=0.1)...\n",
      "Linear(in_features=768, out_features=2, bias=True)...\n"
     ]
    }
   ],
   "source": [
    "num_layers(model, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go into BERT: it has three main parts, the first being BertEmbeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEmbeddings(\n",
       "  (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "  (position_embeddings): Embedding(512, 768)\n",
       "  (token_type_embeddings): Embedding(2, 768)\n",
       "  (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.1)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b0 = get_child(model, 0, 0)\n",
    "b0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main part is the middle: BertEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertEncoder(\n",
       "  (layer): ModuleList(\n",
       "    (0): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (1): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (2): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (3): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (4): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (5): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (6): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (7): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (8): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (9): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (10): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "    (11): BertLayer(\n",
       "      (attention): BertAttention(\n",
       "        (self): BertSelfAttention(\n",
       "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "        (output): BertSelfOutput(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1)\n",
       "        )\n",
       "      )\n",
       "      (intermediate): BertIntermediate(\n",
       "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "      )\n",
       "      (output): BertOutput(\n",
       "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b1 = get_child(model, 0, 1)\n",
    "b1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latter has 12 layers and a gazillion parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, 85054464)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, a simple BertPooler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertPooler(\n",
       "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "  (activation): Tanh()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b2 = get_child(model, 0, 2)\n",
    "b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a custom freezer later that only lets part of BERT train apart from the classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_freezer(model):\n",
    "    unfreeze_model(model)\n",
    "    print('All parameters unfreezed: {}'.format(count_parameters(model)))\n",
    "\n",
    "    ## freeze whole BertLayer\n",
    "    for c in model.children():\n",
    "        if str(c).startswith('Bert'):\n",
    "            freeze_model(c)\n",
    "            \n",
    "    ## unfreeze top 3 layer in BertEncoder\n",
    "    bert_encoder = get_child(model, 0, 1, 0)\n",
    "    for i in range(1, 4):\n",
    "        m = bert_encoder[-i] \n",
    "        print('Unfreezing: {}'.format(m))\n",
    "        unfreeze_model(m)\n",
    "        \n",
    "    ## unfreeze Pooling layer\n",
    "    bert_pooling = get_child(model, 0, 2)\n",
    "    unfreeze_model(bert_pooling)\n",
    "\n",
    "    print('Trainable parameters: {}'.format(count_parameters(model, True)))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All parameters unfreezed: 109483778\n",
      "Unfreezing: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1)\n",
      "  )\n",
      ")\n",
      "Unfreezing: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1)\n",
      "  )\n",
      ")\n",
      "Unfreezing: BertLayer(\n",
      "  (attention): BertAttention(\n",
      "    (self): BertSelfAttention(\n",
      "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "    (output): BertSelfOutput(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1)\n",
      "    )\n",
      "  )\n",
      "  (intermediate): BertIntermediate(\n",
      "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "  )\n",
      "  (output): BertOutput(\n",
      "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "    (LayerNorm): FusedLayerNorm(torch.Size([768]), eps=1e-12, elementwise_affine=True)\n",
      "    (dropout): Dropout(p=0.1)\n",
      "  )\n",
      ")\n",
      "Trainable parameters: 21855746\n"
     ]
    }
   ],
   "source": [
    "custom_freezer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## put the train and text csv to PATH\n",
    "DATA_PATH = '...'\n",
    "\n",
    "def read_AGNews(DATA_PATH):\n",
    "    train_df = pd.read_csv(DATA_PATH+'train.csv', header=None)\n",
    "    train_df.rename(columns={0: 'label',1: 'title', 2:'text'}, inplace=True)\n",
    "    train_df = pd.concat([train_df, pd.get_dummies(train_df['label'],prefix='label')], axis=1)\n",
    "    \n",
    "    test_df = pd.read_csv(DATA_PATH+'test.csv', header=None)\n",
    "    test_df.rename(columns={0: 'label',1: 'title', 2:'text'}, inplace=True)\n",
    "    test_df = pd.concat([test_df, pd.get_dummies(test_df['label'],prefix='label')], axis=1)\n",
    "    return train_df, test_df\n",
    "\n",
    "train_df, test_df = read_AGNews(DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label_1</th>\n",
       "      <th>label_2</th>\n",
       "      <th>label_3</th>\n",
       "      <th>label_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>Wall St. Bears Claw Back Into the Black (Reuters)</td>\n",
       "      <td>Reuters - Short-sellers, Wall Street's dwindli...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>Carlyle Looks Toward Commercial Aerospace (Reu...</td>\n",
       "      <td>Reuters - Private investment firm Carlyle Grou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil and Economy Cloud Stocks' Outlook (Reuters)</td>\n",
       "      <td>Reuters - Soaring crude prices plus worries\\ab...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Iraq Halts Oil Exports from Main Southern Pipe...</td>\n",
       "      <td>Reuters - Authorities have halted oil export\\f...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>Oil prices soar to all-time record, posing new...</td>\n",
       "      <td>AFP - Tearaway world oil prices, toppling reco...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                              title  \\\n",
       "0      3  Wall St. Bears Claw Back Into the Black (Reuters)   \n",
       "1      3  Carlyle Looks Toward Commercial Aerospace (Reu...   \n",
       "2      3    Oil and Economy Cloud Stocks' Outlook (Reuters)   \n",
       "3      3  Iraq Halts Oil Exports from Main Southern Pipe...   \n",
       "4      3  Oil prices soar to all-time record, posing new...   \n",
       "\n",
       "                                                text  label_1  label_2  \\\n",
       "0  Reuters - Short-sellers, Wall Street's dwindli...        0        0   \n",
       "1  Reuters - Private investment firm Carlyle Grou...        0        0   \n",
       "2  Reuters - Soaring crude prices plus worries\\ab...        0        0   \n",
       "3  Reuters - Authorities have halted oil export\\f...        0        0   \n",
       "4  AFP - Tearaway world oil prices, toppling reco...        0        0   \n",
       "\n",
       "   label_3  label_4  \n",
       "0        1        0  \n",
       "1        1        0  \n",
       "2        1        0  \n",
       "3        1        0  \n",
       "4        1        0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1c7f7ae198>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAFvxJREFUeJzt3X+M3PWd3/HnOzYkln2HyZFsLdutLcWq4uCGwApcIVULnGChVcypIBlRsHNEPqWkTXSWGidSCxeClEglqSA/rs5hYS6+GESSsw9MkUtYRZGOHybhMI6PsiVuYkC4iR2DE0rk9N0/5uN2up9Z73dnZ3d2zfMhjXbm/f18v/P5Mfi1853vLJGZSJLU7l397oAkafYxHCRJFcNBklQxHCRJFcNBklQxHCRJFcNBklQxHCRJFcNBklSZ3+8OdOu8887LFStWdLXvr3/9axYuXNjbDvXJmTKWM2Uc4FhmqzNlLFMdx7PPPvuLzHzfRO3mbDisWLGCffv2dbXvyMgIQ0NDve1Qn5wpYzlTxgGOZbY6U8Yy1XFExP9o0s7TSpKkiuEgSaoYDpKkiuEgSaoYDpKkyoThEBHviYinI+LvIuJARPxZqa+MiKci4qWIeCAizi71d5fHo2X7irZjfbbUX4yIq9rqw6U2GhFbej9MSdJkNHnn8DZweWZ+GLgAGI6ItcCXgK9k5irgGHBLaX8LcCwzPwB8pbQjIlYD64EPAcPA1yNiXkTMA74GXA2sBm4obSVJfTJhOGTLifLwrHJL4HLgoVLfDlxb7q8rjynbr4iIKPWdmfl2Zv4UGAUuLrfRzHw5M38L7CxtJUl90ugzh/Ib/nPAEWAv8N+BX2XmydLkMLC03F8K/BygbD8O/EF7fcw+49UlSX3S6BvSmfk74IKIWAx8D/hgp2blZ4yzbbx6p4DKDjUiYhOwCWBgYICRkZHTd3wcR44e554du7radyrWLD2n58c8ceJE1/MwE/a/crxRu4EF9HxNpmO+m+jnmjSd76aarku/5noyer0uvZ7rplaeM29GXl+T+vMZmfmriBgB1gKLI2J+eXewDHi1NDsMLAcOR8R84BzgaFv9lPZ9xquPff6twFaAwcHB7PYr5Pfs2MVd+2f+L4ccunGo58ec7X8SYOOWRxq127zmZM/XZDrmu4l+rknT+W6q6br0a64no9fr0uu5buq+4YUz8vpqcrXS+8o7BiJiAfCHwEHgCeC60mwDcOrXi93lMWX79zMzS319uZppJbAKeBp4BlhVrn46m9aH1rt7MThJUnea/Kq2BNherip6F/BgZj4cET8BdkbEF4AfA/eW9vcCfxkRo7TeMawHyMwDEfEg8BPgJHBrOV1FRHwSeAyYB2zLzAM9G6EkadImDIfMfB74SIf6y7SuNBpb/1/A9eMc607gzg71PcCeBv2VJM0AvyEtSaoYDpKkiuEgSaoYDpKkiuEgSaoYDpKkiuEgSaoYDpKkiuEgSaoYDpKkiuEgSaoYDpKkiuEgSaoYDpKkiuEgSaoYDpKkiuEgSaoYDpKkiuEgSaoYDpKkiuEgSaoYDpKkiuEgSaoYDpKkiuEgSaoYDpKkyoThEBHLI+KJiDgYEQci4lOlfntEvBIRz5XbNW37fDYiRiPixYi4qq0+XGqjEbGlrb4yIp6KiJci4oGIOLvXA5UkNdfkncNJYHNmfhBYC9waEavLtq9k5gXltgegbFsPfAgYBr4eEfMiYh7wNeBqYDVwQ9txvlSOtQo4BtzSo/FJkrowYThk5muZ+aNy/03gILD0NLusA3Zm5tuZ+VNgFLi43EYz8+XM/C2wE1gXEQFcDjxU9t8OXNvtgCRJUxeZ2bxxxArgB8D5wJ8CG4E3gH203l0ci4ivAk9m5rfKPvcCj5ZDDGfmx0v9JuAS4PbS/gOlvhx4NDPP7/D8m4BNAAMDAxft3LlzcqMtjhw9zutvdbXrlKxZek7Pj3nixAkWLVrU8+P2yv5XjjdqN7CAnq/JdMx3E/1ck6bz3VTTdenXXE9Gr9el13Pd1Mpz5k1pHJdddtmzmTk4Ubv5TQ8YEYuA7wCfzsw3IuIbwB1Alp93AX8MRIfdk87vUvI07eti5lZgK8Dg4GAODQ017f7/554du7hrf+Oh98yhG4d6fsyRkRG6nYeZsHHLI43abV5zsudrMh3z3UQ/16TpfDfVdF36NdeT0et16fVcN3Xf8MIZeX01+q8xIs6iFQw7MvO7AJn5etv2bwIPl4eHgeVtuy8DXi33O9V/ASyOiPmZeXJMe0lSHzS5WimAe4GDmfnltvqStmZ/BLxQ7u8G1kfEuyNiJbAKeBp4BlhVrkw6m9aH1ruzdV7rCeC6sv8GYNfUhiVJmoom7xwuBW4C9kfEc6X2OVpXG11A6xTQIeBPADLzQEQ8CPyE1pVOt2bm7wAi4pPAY8A8YFtmHijH+wywMyK+APyYVhhJkvpkwnDIzB/S+XOBPafZ507gzg71PZ32y8yXaV3NJEmaBfyGtCSpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpYjhIkiqGgySpMmE4RMTyiHgiIg5GxIGI+FSpvzci9kbES+XnuaUeEXF3RIxGxPMRcWHbsTaU9i9FxIa2+kURsb/sc3dExHQMVpLUTJN3DieBzZn5QWAtcGtErAa2AI9n5irg8fIY4GpgVbltAr4BrTABbgMuAS4GbjsVKKXNprb9hqc+NElStyYMh8x8LTN/VO6/CRwElgLrgO2l2Xbg2nJ/HXB/tjwJLI6IJcBVwN7MPJqZx4C9wHDZ9vuZ+beZmcD9bceSJPXBpD5ziIgVwEeAp4CBzHwNWgECvL80Wwr8vG23w6V2uvrhDnVJUp/Mb9owIhYB3wE+nZlvnOZjgU4bsot6pz5sonX6iYGBAUZGRibodWcDC2DzmpNd7TsV3fb3dE6cODEtx+2VpvM8HWvSr3np55r0eg6brstsfg2e0ut16ce/ITBzr69G4RARZ9EKhh2Z+d1Sfj0ilmTma+XU0JFSPwwsb9t9GfBqqQ+NqY+U+rIO7SuZuRXYCjA4OJhDQ0Odmk3onh27uGt/41zsmUM3DvX8mCMjI3Q7DzNh45ZHGrXbvOZkz9dkOua7iX6uSdP5bqrpuvRrriej1+vS67lu6r7hhTPy+mpytVIA9wIHM/PLbZt2A6euONoA7Gqr31yuWloLHC+nnR4DroyIc8sH0VcCj5Vtb0bE2vJcN7cdS5LUB01+VbsUuAnYHxHPldrngC8CD0bELcDPgOvLtj3ANcAo8BvgYwCZeTQi7gCeKe0+n5lHy/1PAPcBC4BHy02S1CcThkNm/pDOnwsAXNGhfQK3jnOsbcC2DvV9wPkT9UWSNDP8hrQkqWI4SJIqhoMkqWI4SJIqhoMkqWI4SJIqhoMkqWI4SJIqhoMkqWI4SJIqhoMkqWI4SJIqhoMkqWI4SJIqhoMkqWI4SJIqhoMkqWI4SJIqhoMkqWI4SJIqhoMkqWI4SJIqhoMkqWI4SJIqhoMkqWI4SJIqE4ZDRGyLiCMR8UJb7faIeCUiniu3a9q2fTYiRiPixYi4qq0+XGqjEbGlrb4yIp6KiJci4oGIOLuXA5QkTV6Tdw73AcMd6l/JzAvKbQ9ARKwG1gMfKvt8PSLmRcQ84GvA1cBq4IbSFuBL5VirgGPALVMZkCRp6iYMh8z8AXC04fHWATsz8+3M/CkwClxcbqOZ+XJm/hbYCayLiAAuBx4q+28Hrp3kGCRJPRaZOXGjiBXAw5l5fnl8O7AReAPYB2zOzGMR8VXgycz8Vml3L/BoOcxwZn681G8CLgFuL+0/UOrLgUdPPU+HfmwCNgEMDAxctHPnzkkPGODI0eO8/lZXu07JmqXn9PyYJ06cYNGiRT0/bq/sf+V4o3YDC+j5mkzHfDfRzzVpOt9NNV2Xfs31ZPR6XXo9102tPGfelMZx2WWXPZuZgxO1m9/l8b8B3AFk+XkX8MdAdGibdH6Hkqdp31FmbgW2AgwODubQ0NCkOn3KPTt2cdf+bofevUM3DvX8mCMjI3Q7DzNh45ZHGrXbvOZkz9dkOua7iX6uSdP5bqrpuvRrriej1+vS67lu6r7hhTPy+urqv8bMfP3U/Yj4JvBweXgYWN7WdBnwarnfqf4LYHFEzM/Mk2PaS5L6pKtLWSNiSdvDPwJOXcm0G1gfEe+OiJXAKuBp4BlgVbky6WxaH1rvztY5rSeA68r+G4Bd3fRJktQ7E75ziIhvA0PAeRFxGLgNGIqIC2idAjoE/AlAZh6IiAeBnwAngVsz83flOJ8EHgPmAdsy80B5is8AOyPiC8CPgXt7NjpJUlcmDIfMvKFDedx/wDPzTuDODvU9wJ4O9ZdpXc0kSZol/Ia0JKliOEiSKoaDJKliOEiSKoaDJKliOEiSKoaDJKliOEiSKoaDJKliOEiSKoaDJKliOEiSKoaDJKliOEiSKoaDJKliOEiSKoaDJKliOEiSKoaDJKliOEiSKoaDJKliOEiSKoaDJKliOEiSKoaDJKkyYThExLaIOBIRL7TV3hsReyPipfLz3FKPiLg7IkYj4vmIuLBtnw2l/UsRsaGtflFE7C/73B0R0etBSpImp8k7h/uA4TG1LcDjmbkKeLw8BrgaWFVum4BvQCtMgNuAS4CLgdtOBUpps6ltv7HPJUmaYROGQ2b+ADg6prwO2F7ubweubavfny1PAosjYglwFbA3M49m5jFgLzBctv1+Zv5tZiZwf9uxJEl90u1nDgOZ+RpA+fn+Ul8K/Lyt3eFSO139cIe6JKmP5vf4eJ0+L8gu6p0PHrGJ1ikoBgYGGBkZ6aKLMLAANq852dW+U9Ftf0/nxIkT03LcXmk6z9OxJv2al36uSa/nsOm6zObX4Cm9Xpd+/BsCM/f66jYcXo+IJZn5Wjk1dKTUDwPL29otA14t9aEx9ZFSX9ahfUeZuRXYCjA4OJhDQ0PjNT2te3bs4q79vc7FiR26cajnxxwZGaHbeZgJG7c80qjd5jUne74m0zHfTfRzTZrOd1NN16Vfcz0ZvV6XXs91U/cNL5yR11e3p5V2A6euONoA7Gqr31yuWloLHC+nnR4DroyIc8sH0VcCj5Vtb0bE2nKV0s1tx5Ik9cmEvxJExLdp/dZ/XkQcpnXV0ReBByPiFuBnwPWl+R7gGmAU+A3wMYDMPBoRdwDPlHafz8xTH3J/gtYVUQuAR8tNktRHE4ZDZt4wzqYrOrRN4NZxjrMN2Nahvg84f6J+SJJmjt+QliRVDAdJUsVwkCRVDAdJUsVwkCRVDAdJUsVwkCRVDAdJUsVwkCRVDAdJUsVwkCRVDAdJUsVwkCRVDAdJUsVwkCRVDAdJUsVwkCRVDAdJUsVwkCRVDAdJUsVwkCRVDAdJUsVwkCRVDAdJUsVwkCRVDAdJUmVK4RARhyJif0Q8FxH7Su29EbE3Il4qP88t9YiIuyNiNCKej4gL246zobR/KSI2TG1IkqSp6sU7h8sy84LMHCyPtwCPZ+Yq4PHyGOBqYFW5bQK+Aa0wAW4DLgEuBm47FSiSpP6YjtNK64Dt5f524Nq2+v3Z8iSwOCKWAFcBezPzaGYeA/YCw9PQL0lSQ5GZ3e8c8VPgGJDAf87MrRHxq8xc3NbmWGaeGxEPA1/MzB+W+uPAZ4Ah4D2Z+YVS//fAW5n5Hzs83yZa7zoYGBi4aOfOnV31+8jR47z+Vle7Tsmapef0/JgnTpxg0aJFPT9ur+x/5XijdgML6PmaTMd8N9HPNWk63001XZd+zfVk9Hpdej3XTa08Z96UxnHZZZc923amZ1zzu36Glksz89WIeD+wNyL+/jRto0MtT1Ovi5lbga0Ag4ODOTQ0NMnuttyzYxd37Z/q0Cfv0I1DPT/myMgI3c7DTNi45ZFG7TavOdnzNZmO+W6in2vSdL6barou/Zrryej1uvR6rpu6b3jhjLy+pnRaKTNfLT+PAN+j9ZnB6+V0EeXnkdL8MLC8bfdlwKunqUuS+qTrcIiIhRHxe6fuA1cCLwC7gVNXHG0AdpX7u4Gby1VLa4Hjmfka8BhwZUScWz6IvrLUJEl9MpX38QPA9yLi1HH+KjP/S0Q8AzwYEbcAPwOuL+33ANcAo8BvgI8BZObRiLgDeKa0+3xmHp1CvyRJU9R1OGTmy8CHO9R/CVzRoZ7AreMcaxuwrdu+SJJ6y29IS5IqhoMkqWI4SJIqhoMkqWI4SJIqhoMkqWI4SJIqhoMkqWI4SJIqhoMkqWI4SJIqhoMkqWI4SJIqhoMkqWI4SJIqhoMkqWI4SJIqhoMkqWI4SJIqhoMkqWI4SJIqhoMkqWI4SJIqhoMkqWI4SJIqsyYcImI4Il6MiNGI2NLv/kjSO9msCIeImAd8DbgaWA3cEBGr+9srSXrnmhXhAFwMjGbmy5n5W2AnsK7PfZKkd6zZEg5LgZ+3PT5capKkPojM7HcfiIjrgasy8+Pl8U3AxZn5b8a02wRsKg//MfBil095HvCLLvedbc6UsZwp4wDHMludKWOZ6jj+UWa+b6JG86fwBL10GFje9ngZ8OrYRpm5Fdg61SeLiH2ZOTjV48wGZ8pYzpRxgGOZrc6UsczUOGbLaaVngFURsTIizgbWA7v73CdJeseaFe8cMvNkRHwSeAyYB2zLzAN97pYkvWPNinAAyMw9wJ4Zeropn5qaRc6UsZwp4wDHMludKWOZkXHMig+kJUmzy2z5zEGSNIucseEQEdsi4khEvDDO9oiIu8uf63g+Ii6c6T421WAsQxFxPCKeK7f/MNN9bCIilkfEExFxMCIORMSnOrSZE+vScCxzZV3eExFPR8TflbH8WYc2746IB8q6PBURK2a+p6fXcBwbI+J/tq3Jx/vR16YiYl5E/DgiHu6wbXrXJDPPyBvwz4ALgRfG2X4N8CgQwFrgqX73eQpjGQIe7nc/G4xjCXBhuf97wH8DVs/FdWk4lrmyLgEsKvfPAp4C1o5p86+BPy/31wMP9LvfXY5jI/DVfvd1EmP6U+CvOr2OpntNzth3Dpn5A+DoaZqsA+7PlieBxRGxZGZ6NzkNxjInZOZrmfmjcv9N4CD1N+HnxLo0HMucUOb6RHl4VrmN/TByHbC93H8IuCIiYoa62EjDccwZEbEM+OfAX4zTZFrX5IwNhwbOtD/Z8U/L2+lHI+JD/e7MRMpb4I/Q+u2u3Zxbl9OMBebIupTTF88BR4C9mTnuumTmSeA48Acz28uJNRgHwL8spywfiojlHbbPFv8J+HfA/x5n+7SuyTs5HDol7Fz9LeNHtL4S/2HgHuCv+9yf04qIRcB3gE9n5htjN3fYZdauywRjmTPrkpm/y8wLaP11gosj4vwxTebEujQYx98AKzLznwD/lf/3m/esEhH/AjiSmc+erlmHWs/W5J0cDo3+ZMdckJlvnHo7na3vi5wVEef1uVsdRcRZtP4x3ZGZ3+3QZM6sy0RjmUvrckpm/goYAYbHbPq/6xIR84FzmMWnOscbR2b+MjPfLg+/CVw0w11r6lLgoxFxiNZfqb48Ir41ps20rsk7ORx2AzeXq2PWAscz87V+d6obEfEPTp1rjIiLaa3rL/vbq1rp473Awcz88jjN5sS6NBnLHFqX90XE4nJ/AfCHwN+PabYb2FDuXwd8P8snobNFk3GM+fzqo7Q+K5p1MvOzmbksM1fQ+rD5+5n5r8Y0m9Y1mTXfkO61iPg2ratFzouIw8BttD6gIjP/nNa3sa8BRoHfAB/rT08n1mAs1wGfiIiTwFvA+tn2H25xKXATsL+cFwb4HPAPYc6tS5OxzJV1WQJsj9b/dOtdwIOZ+XBEfB7Yl5m7aQXhX0bEKK3fTtf3r7vjajKOfxsRHwVO0hrHxr71tgszuSZ+Q1qSVHknn1aSJI3DcJAkVQwHSVLFcJAkVQwHSVLFcJAkVQwHSVLFcJAkVf4PpwLNpaB/v+QAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.label.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f1ca0051080>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_df.text.apply(lambda x: len(x)).hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The document lengths will be important in a second."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding the text for BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do the following:\n",
    "* apply the pre-trained sub-word tokenizer that comes with BERT,\n",
    "* append special tokens for classification at the beginning and end of each sample,\n",
    "* make sure that each sample sequence of tokens has length at most 512 (that's the max input length for BERT).\n",
    "\n",
    "We will pad everything to uniform length to make life easier (torch.tensor will throw errors otherwise at later steps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3899, 2795]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## tokenizer.encode() will do the tokenization and converting into subword IDs in a single step\n",
    "\n",
    "tokenizer.encode('a dog is not a table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## cutting and padding\n",
    "def padSentenceForBert(sent, cls_id, sep_id, lg):\n",
    "    return [cls_id]+sent[:lg-2]+[sep_id] + [0]*(lg - len(sent[:lg-2])-2) \n",
    "        \n",
    "def padAndTokenizeForBert(sentences, lg):\n",
    "    cls_id = tokenizer.convert_tokens_to_ids(tokenizer.cls_token)\n",
    "    sep_id = tokenizer.convert_tokens_to_ids(tokenizer.sep_token)\n",
    "    return [padSentenceForBert(tokenizer.encode(sent), cls_id, sep_id, lg) for sent in tqdm_notebook(sentences)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1364dac173549b8a4acad39ff86c1c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=120000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_ids = padAndTokenizeForBert(train_df.text, 512) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42cdb9a4e7004792a6f200212ab02c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=7600), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_ids = padAndTokenizeForBert(test_df.text, 512)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful, the **labels need to be from 0 to number_of_classes**. In our case, this is simply done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = train_df.label.values - 1\n",
    "test_labels = test_df.label.values - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make sure that the model ignores the padding, so we define masks that will be fed to the BERT model as well (1 stands for a real token and 0 for padding coordinates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masker(input_ids):\n",
    "    return [[float(i>0) for i in seq] for seq in input_ids]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = masker(train_ids)\n",
    "test_mask = masker(test_ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation split for training data and masks together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_RATIO = 0.1\n",
    "train_ids, val_ids, train_labels, val_labels, train_mask, val_mask = train_test_split(train_ids, train_labels, train_mask,  \n",
    "                                                                                random_state=42, test_size=SPLIT_RATIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108000, 108000, 108000, 12000, 12000, 12000)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ids), len(train_labels), len(train_mask), len(val_ids), len(val_labels), len(val_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feeding the data to BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch has great dataset and dataloader classes that make life easier, especially when the data does not fit into memory. At the same time we construct the loader, we start turning our id sequences to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "def loader(ids, mask, labels, mode = 'random'):\n",
    "    ids = torch.tensor(ids)\n",
    "    mask = torch.tensor(mask)\n",
    "    labels = torch.tensor(labels)\n",
    "    train_data = TensorDataset(ids, mask, labels)\n",
    "    if mode == 'random': \n",
    "        train_sampler = RandomSampler(train_data)\n",
    "    else:\n",
    "        train_sampler = SequentialSampler(train_data)\n",
    "    return DataLoader(train_data, sampler=train_sampler, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = loader(train_ids, train_mask, train_labels)\n",
    "val_loader = loader(val_ids, val_mask, val_labels, mode = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can look inside these data loaders as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids, mask, label = batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 512]), torch.Size([32, 512]), torch.Size([32]))"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids.size(), mask.size(), label.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the batch of 32 samples, each of length 512."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The BERT sequence classifier outputs logits which need to be turned into probabilities (softmax) before fed into the metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, roc_curve, auc, precision_score, recall_score\n",
    "\n",
    "## alternatively: from scipy.special import softmax\n",
    "\n",
    "def softmax(logits):\n",
    "    return np.array([a/b for a,b in zip(np.exp(logits), np.sum(np.exp(logits), axis=1))])\n",
    "\n",
    "def softmax_accuracy(labels, logits):\n",
    "    preds = np.argmax(softmax(logits), axis=1)\n",
    "    return accuracy_score(labels, preds)\n",
    "\n",
    "def softmax_rocauc(labels, logits):\n",
    "    preds = np.argmax(softmax(logits), axis=1)\n",
    "    return roc_auc_score(labels, preds)\n",
    "\n",
    "def save_checkpoint(state, is_best, logdir = ''):\n",
    "    statefilename = logdir+'last_checkpoint.pt'\n",
    "    torch.save(state, statefilename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(statefilename, logdir+'model_best.pt')\n",
    "\n",
    "def eval_metrics(metrics, labels, preds):\n",
    "    result = dict()\n",
    "    for name, metric in metrics.items():\n",
    "        result[name] = metric(labels, preds)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will have a train-method which takes the model, optimizer and learning rate scheduler with a hyperparameter dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict()\n",
    "params['epochs'] = 4\n",
    "params['max_grad'] = 1\n",
    "params['base_lr'] = 1e-3\n",
    "params['metrics'] = {'accuracy': softmax_accuracy, 'roc_auc': softmax_rocauc}\n",
    "\n",
    "## where to save the models (they can take a few Gb if all the params are training)\n",
    "LOG_PATH = '...'\n",
    "\n",
    "params['logdir'] = LOG_PATH + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")+\"/\"\n",
    "\n",
    "params['train_dataloader'] = train_loader\n",
    "params['val_dataloader'] = val_loader\n",
    "\n",
    "num_total_steps = params['epochs']*len(params['train_dataloader'])\n",
    "warmup_proportion = 0.1\n",
    "num_warmup_steps = int(num_total_steps*warmup_proportion)\n",
    "    \n",
    "optimizer = AdamW(model.parameters(), lr=params['base_lr'], correct_bias=False) \n",
    "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=num_warmup_steps, t_total=num_total_steps) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the train-method that I still need to modularize:\n",
    "* we set the model and opt. to mixed precision for faster training,\n",
    "* the main for-loop goes over the epochs:\n",
    "    * first, we loop over each batch, calculate the loss and back propagate,\n",
    "    * second, we evaluate the model on the validation set with respect to our metrics,\n",
    "    * last, we save the current model and update the best_model (if needed).\n",
    "    \n",
    "The method will return the history dictionary (much like Keras) with a bunch of nice metrics we can visualize.     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(mops, params, do_amp = True, test_mode=False, warm = False):\n",
    "    \n",
    "    model, optimizer, scheduler = mops\n",
    "    print('Trainable params in the model: {}\\n'.format(count_parameters(model)))\n",
    "    \n",
    "    ## metrics and logging\n",
    "    metrics = params['metrics']\n",
    "    os.makedirs(params['logdir'], exist_ok=True)\n",
    "    history = defaultdict(list)\n",
    "    \n",
    "  \n",
    "    ## If new training started we reset the mixed-precision training:\n",
    "    if not warm and do_amp:\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O2\", \n",
    "                                              keep_batchnorm_fp32=True, loss_scale=\"dynamic\")\n",
    "\n",
    "\n",
    "    for epoch in range(params['epochs']):\n",
    "        print(\"Epoch {}:\".format(epoch+1))\n",
    "        ## set training mode!\n",
    "        model.train()\n",
    "\n",
    "        ## Tracking loss during the epoch\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "        ## Train the data for one epoch\n",
    "\n",
    "        for batch in tqdm_notebook(params['train_dataloader']):\n",
    "            \n",
    "            ## Add batch to GPU and unpack\n",
    "            batch = tuple(t.to(device) for t in batch)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "            ## Clear out the gradients (by default they accumulate) and forward pass\n",
    "            optimizer.zero_grad()\n",
    "            output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, \n",
    "                           labels=b_labels)\n",
    "            loss = output[0]\n",
    "\n",
    "            ## Backward pass\n",
    "            if do_amp:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            ## Update parameters and clip\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), params['max_grad'])  \n",
    "\n",
    "\n",
    "            ## logging\n",
    "            history['train_loss_per_batch'].append(loss.item()) \n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "            \n",
    "            ## for testing, break early\n",
    "            if test_mode and nb_tr_steps > 20:\n",
    "                break\n",
    "        \n",
    "        ## logging\n",
    "        history['train_loss_per_epoch'].append(tr_loss/nb_tr_steps)\n",
    "        \n",
    "        print(\"Train loss: {}\".format(history['train_loss_per_epoch'][-1]))\n",
    "\n",
    "        \n",
    "        print('Validation...')\n",
    "\n",
    "        val_labels, val_logits = testEval(model, params['val_dataloader'], test_mode = test_mode)\n",
    "    \n",
    "        for name, metric in metrics.items():\n",
    "            m = metric(val_labels, val_logits)\n",
    "            history['val_'+name].append(m)\n",
    "            print(\"Validation {}: {}\".format(name, m))\n",
    "        \n",
    "        ## Saving the history file if training is interrupted early\n",
    "        with open(logdir+'history.pk', 'wb') as f:\n",
    "            f.truncate(0)\n",
    "            pickle.dump(history, f)\n",
    "                \n",
    "        ## Saving best (wrt monitored quantity) and also the last model \n",
    "        monitored = history['val_accuracy']\n",
    "        best_value = max(monitored)\n",
    "        last_value = monitored[-1]\n",
    "        is_last_best = last_value == best_value\n",
    "        \n",
    "        save_checkpoint({\n",
    "            'epoch': epoch + 1,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict' : optimizer.state_dict(),\n",
    "            'best_kpi': best_value,\n",
    "            'loss': history['train_loss_per_epoch'][-1],\n",
    "            #'params': params\n",
    "        }, is_last_best, logdir)\n",
    "   \n",
    "    return history\n",
    "\n",
    "def testEval(model, dataloader, test_mode=False):\n",
    "    ## eval mode!\n",
    "    model.eval()\n",
    "\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    val_logits = []\n",
    "    val_labels = []\n",
    "\n",
    "    for batch in tqdm_notebook(dataloader):\n",
    "        ## Add batch to GPU and unpack\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "\n",
    "        ## not calculating the gradients speeds up inference a lot\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "        ## logits and labels to CPU\n",
    "        logits = logits[0].detach().cpu().tolist()\n",
    "        label_ids = b_labels.to('cpu').tolist()\n",
    "\n",
    "        val_logits += logits\n",
    "        val_labels += label_ids\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "        ## if testing break early\n",
    "        if test_mode and nb_eval_steps > 20:\n",
    "            break   \n",
    "            \n",
    "    return val_labels, val_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable params in the model: 21855746\n",
      "\n",
      "Selected optimization level O2:  FP16 training with FP32 batchnorm and FP32 master weights.\n",
      "\n",
      "Defaults for this optimization level are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n",
      "Processing user overrides (additional kwargs that are not None)...\n",
      "After processing overrides, optimization options are:\n",
      "enabled                : True\n",
      "opt_level              : O2\n",
      "cast_model_type        : torch.float16\n",
      "patch_torch_functions  : False\n",
      "keep_batchnorm_fp32    : True\n",
      "master_weights         : True\n",
      "loss_scale             : dynamic\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found param bert.embeddings.word_embeddings.weight with type torch.cuda.HalfTensor, expected torch.cuda.FloatTensor.\nWhen using amp.initialize, you do not need to call .half() on your model\nbefore passing it, no matter what optimization level you choose.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-22bcadedcbcd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-91-ce8a3d4a4b35>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(mops, params, do_amp, test_mode, warm)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mwarm\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdo_amp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         model, optimizer = amp.initialize(model, optimizer, opt_level=\"O2\", \n\u001b[0;32m---> 15\u001b[0;31m                                               keep_batchnorm_fp32=True, loss_scale=\"dynamic\")\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/apex/amp/frontend.py\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(models, optimizers, enabled, opt_level, cast_model_type, patch_torch_functions, keep_batchnorm_fp32, master_weights, loss_scale, cast_model_outputs, num_losses, verbosity, min_loss_scale, max_loss_scale)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[0mmaybe_print\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{:22} : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_amp_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopt_properties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcast_model_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/apex/amp/_initialize.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(models, optimizers, properties, num_losses, cast_model_outputs)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_amp_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_incoming_model_not_fp32\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mcheck_params_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/apex/amp/_initialize.py\u001b[0m in \u001b[0;36mcheck_params_fp32\u001b[0;34m(models)\u001b[0m\n\u001b[1;32m     82\u001b[0m                         \u001b[0;34m\"When using amp.initialize, you do not need to call .half() on your model\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m                         \"before passing it, no matter what optimization level you choose.\".format(\n\u001b[0;32m---> 84\u001b[0;31m                         name, param.type()))\n\u001b[0m\u001b[1;32m     85\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_cuda\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                     warn_or_err(\"Found param {} with type {}, expected torch.cuda.FloatTensor.\\n\"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/apex/amp/_amp_state.py\u001b[0m in \u001b[0;36mwarn_or_err\u001b[0;34m(msg)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Warning:  \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0;31m# I'm not sure if allowing hard_override is a good idea.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m# + \"  If you're sure you know what you're doing, supply \" +\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found param bert.embeddings.word_embeddings.weight with type torch.cuda.HalfTensor, expected torch.cuda.FloatTensor.\nWhen using amp.initialize, you do not need to call .half() on your model\nbefore passing it, no matter what optimization level you choose."
     ]
    }
   ],
   "source": [
    "history = train((model, optimizer, scheduler), params, test_mode=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks about the hyper-parameters:\n",
    "* try out different learning rate schedules (cyclic works best a lot of times) but be careful about large values and catastrophic forgetting;\n",
    "* freeze and unfreeze layers, and adapt the learning rate to decrease on lower layers;\n",
    "* the recommended batch size for BERT fine-tuning is 16 or 32 (memory is also a big issue);\n",
    "* the model should be done in 4-6 epoch usually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_history(history, metrics):\n",
    "    plt.figure(figsize=(15,8))\n",
    "    plt.title(\"Training loss\")\n",
    "    plt.xlabel(\"Batch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.plot(history['train_loss_per_batch'])\n",
    "    plt.show()\n",
    "\n",
    "    plt.title(\"Train loss and val acc\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss/Acc\")\n",
    "    for name in metrics.keys():\n",
    "        plt.plot(history['val_'+name])\n",
    "    plt.legend(['Train loss']+ [name for name in metrics.keys()])\n",
    "    plt.show()\n",
    "    \n",
    "draw_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, (finding the best hyper parameters and retraining the model a dozen times) we can reload the best model and evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, _ = loadModelTokenizer(4, 'english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = ...\n",
    "\n",
    "checkpoint = torch.load(PATH)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = loader(test_ids, test_mask, test_labels)\n",
    "\n",
    "labels, logits = testEval(model, test_loader)\n",
    "\n",
    "metrics = {'accuracy': softmax_accuracy, 'roc_auc': softmax_rocauc}\n",
    "\n",
    "for name, metric in metrics.items():\n",
    "    m = metric(val_labels, val_logits)\n",
    "    print(\"Test {}: {}\".format(name, m))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
